# üìä Ray Cluster - Reporte de Problema de Scheduling

**Fecha**: 2026-01-28
**Versi√≥n Ray**: 2.51.2
**Plataforma**: macOS (Darwin 25.1.0)

---

## ‚úÖ Estado Actual del Cluster

### Configuraci√≥n (CORRECTA)
- **MacBook Pro (100.77.179.14)**: Head Node - 12 CPUs
- **MacBook Air (100.118.215.73)**: Worker Node - 10 CPUs
- **Total**: 22 CPUs disponibles
- **Conectividad**: Estable v√≠a Tailscale VPN
- **Python**: 3.9.6 en ambas m√°quinas
- **Ray**: 2.51.2 en ambas m√°quinas

### Verificaciones Exitosas
‚úÖ Ambos nodos vivos y registrados en GCS
‚úÖ PYTHONPATH configurado correctamente en Worker
‚úÖ Ambos nodos pueden ejecutar tareas (confirmado con tests)
‚úÖ No hay errores cr√≠ticos en logs

---

## ‚ùå PROBLEMA IDENTIFICADO: Distribuci√≥n Desbalanceada de Tareas

### Comportamiento Observado
- **Head (MacBook Pro)**: 99.3% de las tareas (298/300)
- **Worker (MacBook Air)**: 0.7% de las tareas (2/300)

### Tests Realizados
1. **Test peque√±o** (10 poblaci√≥n, 5 generaciones = 50 tareas)
   - Distribuci√≥n: 93.5% Head, 6.5% Worker

2. **Test mediano** (100 poblaci√≥n, 10 generaciones = 1000 tareas)
   - Distribuci√≥n: 99.3% Head, 0.7% Worker
   - Tiempo: 102 segundos (9.8 backtests/segundo)

### Causa Ra√≠z
**Ray 2.51.2 en macOS NO respeta `scheduling_strategy="SPREAD"`**

Evidencia:
- C√≥digo ya tiene `.options(scheduling_strategy="SPREAD")` en strategy_miner.py l√≠nea 240
- Se agreg√≥ tambi√©n en optimizer.py l√≠nea 84: `@ray.remote(num_cpus=1, scheduling_strategy="SPREAD")`
- A pesar de esto, Ray asigna casi todas las tareas al Head

Este es un **bug conocido/limitaci√≥n de Ray en clusters macOS**. Ray est√° optimizado para Linux, y el scheduling en macOS tiene comportamiento sub√≥ptimo.

---

## üîß SOLUCIONES

### Opci√≥n 1: Actualizar Ray ‚≠ê (RECOMENDADO)

Ray 2.51.2 es de mayo 2024. Versiones m√°s recientes pueden tener mejoras en scheduling para macOS.

**Pasos:**
```bash
# 1. En MacBook Air (Worker)
ssh enderj@100.118.215.73
pip3 install --upgrade ray
ray stop --force

# 2. En MacBook Pro (Head)
pip3 install --upgrade ray
ray stop --force

# 3. Reiniciar Head
ray start --head --port=6379 --node-ip-address=100.77.179.14 --num-cpus=12

# 4. Reiniciar Worker
launchctl load ~/Library/LaunchAgents/com.bittrader.worker.plist
```

**Ventajas:**
- Soluci√≥n oficial
- Puede incluir correcciones de bugs
- Mejoras de rendimiento

**Desventajas:**
- Requiere actualizaci√≥n en ambas m√°quinas
- Posibles breaking changes

---

### Opci√≥n 2: Usar Placement Groups (Workaround Avanzado)

Forzar distribuci√≥n expl√≠cita usando Ray Placement Groups.

**Implementaci√≥n requerida en strategy_miner.py:**
```python
import ray

# Crear placement group al iniciar
pg = ray.util.placement_group([
    {"CPU": 12, "node:100.77.179.14": 1},  # Head bundle
    {"CPU": 10, "node:100.118.215.73": 1}   # Worker bundle
], strategy="STRICT_SPREAD")

ray.get(pg.ready())

# Al lanzar tareas, usar el placement group
futures = [
    run_backtest_task.options(
        placement_group=pg,
        placement_group_bundle_index=i % 2  # Alterna entre Head y Worker
    ).remote(...)
    for i, genome in enumerate(population)
]
```

**Ventajas:**
- Control expl√≠cito de distribuci√≥n
- No requiere actualizar Ray

**Desventajas:**
- C√≥digo m√°s complejo
- Requiere modificaciones significativas
- Puede tener overhead

---

### Opci√≥n 3: Aceptar Desbalance y Optimizar

El cluster funciona, solo est√° desbalanceado. Dado que el Head tiene M√ÅS CPUs (12 vs 10), podr√≠a ser aceptable.

**An√°lisis:**
- Head: 12 CPUs (54.5% de capacidad)
- Worker: 10 CPUs (45.5% de capacidad)
- Distribuci√≥n actual: 99% Head vs 1% Worker ‚ùå

**Mejora posible sin c√≥digo:**
```bash
# Reducir CPUs del Head para forzar overflow al Worker
ray stop --force
ray start --head --port=6379 --node-ip-address=100.77.179.14 --num-cpus=6  # Reducir de 12 a 6
```

Esto forza a Ray a usar el Worker cuando el Head se satura (6 CPUs).

**Ventajas:**
- Sin cambios de c√≥digo
- Soluci√≥n inmediata

**Desventajas:**
- Desperdicia CPUs del Head
- No resuelve el problema real
- 6+10=16 CPUs totales en lugar de 22

---

### Opci√≥n 4: Migrar a Linux (Soluci√≥n Definitiva)

Ray funciona √≥ptimamente en Linux. Si el proyecto crece, considerar:
- Raspberry Pi Cluster
- AWS EC2 spot instances
- Servidor Linux local

**Ventajas:**
- Scheduling perfecto
- Mejor rendimiento general
- M√°s escalable

**Desventajas:**
- Requiere nueva infraestructura
- Costo adicional (si cloud)
- Tiempo de migraci√≥n

---

## üìã PR√ìXIMOS PASOS RECOMENDADOS

### Corto Plazo (HOY)
1. ‚úÖ Cluster estabilizado (HECHO)
2. ‚ö†Ô∏è Decidir entre Opci√≥n 1 (actualizar Ray) u Opci√≥n 3 (reducir CPUs Head)

### Mediano Plazo
1. Si actualizar Ray no resuelve, implementar Placement Groups (Opci√≥n 2)
2. Monitorear rendimiento y distribuci√≥n en optimizaciones largas

### Largo Plazo
1. Evaluar migraci√≥n a Linux si scheduling sigue siendo problema
2. Considerar escalar con m√°s Workers

---

## üìù NOTAS T√âCNICAS

### Archivos Modificados
- `optimizer.py` l√≠nea 84: Agregado `scheduling_strategy="SPREAD"` a `@ray.remote`
- `strategy_miner.py` l√≠nea 300: Agregado `scheduling_strategy="SPREAD"` a reintentos

### Logs de Debugging
- `/tmp/test_load_output.log`: Test de 1000 backtests
- `/tmp/test_spread_validation.log`: Validaci√≥n post-fix
- `~/.bittrader_worker/logs/worker.log`: Logs del Worker

### Comandos de Verificaci√≥n
```bash
# Ver estado del cluster
ssh enderj@100.77.179.14 "python3 /tmp/check_ray_status.py"

# Ver distribuci√≥n de tareas
grep 'ip=100' /tmp/test_load_output.log | sort | uniq -c
```

---

## üéØ RECOMENDACI√ìN FINAL

**Intentar Opci√≥n 1 (Actualizar Ray) primero**. Si no mejora, implementar Opci√≥n 3 (reducir CPUs Head) como workaround temporal mientras se planifica Opci√≥n 2 (Placement Groups) para soluci√≥n robusta.

El cluster funciona correctamente desde el punto de vista t√©cnico. El √∫nico problema es el scheduling sub√≥ptimo de Ray en macOS, que tiene soluciones conocidas.

---

**Contacto**: Claude Sonnet 4.5
**Sesi√≥n**: 2026-01-28 22:00-23:00
